# 强化学习如何在大模型中应用？（2）

[TOC]

本文将在前文基础上以PPO为例，展开RLHF在NLP或LLM语境下的整体框架

## 1. RLHF-PPO的四个模型

RLHF-PPO包含四个模型，其中**Actor/Critic Model**需要训练，**Reward/Reference Model**不用训练

**Actor Model**：策略模型，要训练的目标语言模型，Actor模型直接用SFT训练出的模型进行初始化

**Critic Model**：评论家模型，预计期望总收益，Critic模型在SFT模型的基础上加数值头后训练得到

**Reward Model**：奖励模型，计算即时收益，Reward模型在SFT模型的基础上加数值头后训练得到（这里是指奖励模型训练，不是指RLHF-PPO训练）

**Reference Model**：参考模型，它的作用是在RLHF阶段给语言模型增加一些“约束”，防止语言模型训歪。Reference 模型直接用SFT训练出的模型进行初始化

### 1.1 策略模型 / Actor Model（要优化的语言模型）

在给定 prompt（前缀） $s_t$ 下产生下一个 token（策略 $\pi_\theta$），并通过 PPO 迭代更新，使**整段**回复的回报最大

Actor模型直接用SFT训练出的模型进行初始化

训练目的是让Actor模型能产生符合人类喜好的回复，输入给Actor模型用户查询，将产生的回复和用户查询一起计算损失，用于更新Actor模型

那Actor Model的Loss该如何计算呢？

（1）初版Actor Model Loss

强化学习的核心目标是最大化累积奖励：
$$
\begin{equation}\max_\pi\mathbb{E}_{\tau\sim\pi}\left[\sum_{t=0}^T\gamma^tr(s_t,a_t)\right]\end{equation}
$$
其中，$$V_t$$表示t时刻预期收益，$$\pi (a_t | s_t)$$表示采取$$a_t$$动作的概率

由上，Actor Model的损失可以定义为：
$$
\begin{equation}Loss_{Actor}=-\sum_t^TV_t\log\pi(a_t|s_t)\end{equation}
$$
当Critic评估的状态价值$$V_t$$为正时，应该增加该动作的概率

当$$V_t$$为负时，应该减少该动作概率

接下来只考虑t时刻的损失函数，省略求和符号

## 4. References

[1]

