# 强化学习如何在大模型中应用？（1）

[TOC]

## 1. 前置知识

关于强化学习的基础知识，强烈建议大家去b站听一下西湖大学赵世钰老师的课，深入浅出，很适合大家入门

链接附上：[【强化学习的数学原理】课程：从零开始到透彻理解（完结）]( https://www.bilibili.com/video/BV1sd4y167NS/?share_source=copy_web&vd_source=ed92016b5f2fd5c0ff7689574a11240e)

本文不会在基础知识上花费太多的篇幅，只有强相关的知识会简单提一嘴，建议大家还是先把基础打好，更有利于构建整体的知识体系

之前我们从 Paper2Code 角度入手，也讲解了一小部分常用的强化学习算法，可以持续关注我的专栏[【持续更新】RLHF4LLMs/Agent](https://blog.csdn.net/weixin_60186742/category_13102692.html)

但初始学习过程中始终有一个问题困扰着我：**强化学习到底是如何应用到LLM中的**？

我们知道RLHF用于大语言模型的后训练，所以学了一堆传统强化学习的知识，希望能融会贯通

学习结束后却发现知识之间仿佛出现了巨大的鸿沟

要理清两者之间的关联，首先我们要明确：**RL和RLHF的区别和关系是什么？**

## 2. RL&RLHF

**RL（Reinforcement Learning）**，也就是传统意义的强化学习，其通常是**和环境交互、用奖励函数学策略**

比如给定环境 $S,A,P,R$，智能体反复交互，最大化长期回报

**RLHF（Reinforcement Learning Human Feedback）**，从人类反馈中强化学习

可以看到和RL的区别就是，后面加了个 Human Feedback，那为什么要加**人类反馈**呢？人类反馈又是**加在哪里**？

在强化学习过程中，最难的是怎么写**奖励函数**（设计奖励模型）

**RL** 假设有一个**可编程、可反复交互的环境**和**清晰的数值奖励**

而聊天、写作、礼貌与安全这些目标**很难用规则打分**

所以在大语言模型业内更常说 **RLHF**，点明“奖励来自人类偏好”，也更准确地指代一整套后训练流程

LLM用人工对比标注学**奖励模型**或直接做**偏好优化**（GRPO），这就是 RLHF 的 HF

说 RL 也容易让人联想到 Atari/机器人/推荐等传统场景与算法族

说RLHF能一眼看出是**大模型**的那条线

那传统的强化学习流程到底是如何应用到NLP任务中的呢？

换个角度说，NLP任务中的**智能体（Agent）、环境（Environment）、状态（State）、动作（Action）**分别对应什么？

## 3.  NLP任务中的强化学习

首先我们要明确两点：

（1）对NLP任务做强化学习的目的是：**生成增强**，即给定模型prompt，模型能生成符合人类喜好的回答

（2）同时NLP模型在做推理时，通常是自回归的，即**每个时刻t只产生一个token，先有上一个token，再有下一个token**

### 3.1 NLP中的对应关系

根据这两点，我们就可以回答在上一节提出的问题：

**智能体（agent）**＝ 语言模型

**策略** ＝ 模型在每个前缀下对所有词的概率分布 $\pi_\theta(a_t|s_t)$

**状态 $s_t$**＝ 当前上下文/前缀（用户 prompt + 之前已生成的 token）

**动作 $a_t$** ＝ 选下一个 token（动作空间≈词表）

**转移**：选出 $a_t$ 后，前缀变成 $s_{t+1}=s_t \oplus a_t$，直到生成 <eos>/到达长度上限，形成完整回复 $y$

### 3.2 NLP中的句子级奖励

在**RLHF**里，常见做法是：

对整段回复 $y$ 用**奖励模型 RM**给一个分数（人类更喜欢的程度），再用 **PPO** 等算法做策略优化，并加上**KL 正则**约束模型别偏离基座语言模型太远，以防“骗奖励”，以PPO举例，具体的形式可以写成：
$$
\begin{equation}R(y)=\mathrm{RM}(y)-\beta\mathrm{KL}(\pi_\theta(\cdot|s)\parallel\pi_{\mathrm{ref}}(\cdot|s))\end{equation}
$$
再把这个**句子级奖励**通过优势函数分配回每个 token 的梯度

并不是每个 token 都真的拿到一个“显式即刻奖励”

这套流程最早系统化地用在人类偏好上的是[Deep Reinforcement Learning from Human Preferences](https://papers.neurips.cc/paper/7017-deep-reinforcement-learning-from-human-preferences.pdf)：先学一个奖励模型，再用 RL 优化策略

后来的 **InstructGPT** :[Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155)，把它用于指令跟随的语言模型，并用 PPO+KL 成功对齐模型行为

### 3.3 “环境”怎么理解更贴切？

最小化地说，**环境**就是“把你选的 token 接到前缀上并告诉你是否结束”，下一步的**观测**就是更新后的前缀

若是工具使用、可浏览网页、与人互动的设定，环境还包括检索结果、网页、以及人类反馈

**环境 = 前缀增长与（可能的）外部交互 + 反馈机制**

### 3.4 “优化评测指标”的传统 RL 在 NLP 里的关系

在没有人类偏好时，也常把**整句的 BLEU/ROUGE/CIDEr**当奖励，用 REINFORCE/SCST 直接优化

同样是“状态 = 前缀、动作 = 词、奖励 = 整句指标”

这与上面的映射完全一致，只是换了奖励来源

### 3.5 $A_t$ 是语言模型产出的，那 $R_t$、$V_t$ 从哪来？

**$A_t$**：策略 $\pi_\theta$（语言模型）在前缀 $s_t$ 下采样/解码出的**下一个token**。

**$R$**：来自**外部的奖赏机制**；在RLHF中，通常是**奖励模型（RM）**给整段回复打分，并加上**KL 约束惩罚**，常用目标形如 $R(y)=\text{RM}(y)-\beta\,\mathrm{KL}(\pi_\theta\|\pi_{\rm ref})$。

**$V_t$**：**价值函数**（critic/value head）对“从当前前缀继续生成下去的期望总回报”的估计，用来做基线/优势函数，降低方差（如GAE）

**这部分内容我将在下篇文章中以PPO为例详细展开，介绍LLM的RLHF整体架构**

### 3.6 什么时候更新参数？看到一个 $R_t$ 就更新吗？

当然不会

主流做法（如 **PPO**）是：**先收集一批完整轨迹**（完整的回复），再基于这些样本计算优势并**做多轮小批次更新**

这比每出一个token就更新稳定得多，也更符合序列级奖励的设定

InstructGPT 等RLHF系统就是 SFT → 收集偏好/训练RM → 用 **PPO+KL** 在批上更新

### 3.7 在NLP里怎么理解 $R_t$ 和 $V_t$？

**$R_t$**（即时收益）在很多NLP/RLHF实现里**绝大多数时间为0**，只在**终止步**给一次回合奖励

KL项常按token分摊，起到“致密奖励/正则化”作用。

**$V_t$** 是**长期（到句末）的期望回报**，不是眼前一个token的好坏

训练时常用GAE把终止奖励平滑地分配回各步以做信用分配

**有限长度生成里折扣常取 $\gamma\approx1$**

### 3.8 必须每个token都是动作吗？

经典做法是的（动作空间 ≈ 词表），也可以定义更高层动作，但主流RLHF/指标直优化都用token级动作

## 4. Preview

下篇文章我会以PPO为例，详细展开RLHF的四个重要角色——Actor Model/Critic Model/Reward Model/Reference Model，并讲解他们在NLP或LLM的语境中是如何训练的

## 5. References

[1]知乎：[图解大模型RLHF系列之：人人都能看懂的PPO原理与源码解读](https://zhuanlan.zhihu.com/p/677607581)

[2]arXiv:[Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155)

[3] NeurIPS论文集：[Deep Reinforcement Learning from Human Preferences](https://papers.neurips.cc/paper/7017-deep-reinforcement-learning-from-human-preferences.pdf)

