{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码实现ppo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先把本教程中的mask忽略，加入了一些mask写的有点乱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trl代码中的对于ppo的实现\n",
    "https://github.com/huggingface/trl/blob/main/trl/trainer/ppo_trainer.py\n",
    "\n",
    "https://mp.weixin.qq.com/s/S72LO26IsZ8AED8sQKIWnQ\n",
    "\n",
    "讲了PPO  loss max https://zhuanlan.zhihu.com/p/28223597805\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/677607581"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 基本超参（教学用简化配置） ----\n",
    "# 这些超参用于快速搭建一个极小的 GPT-2/ppo 教学环境，便于直观看到张量形状和算法流程。\n",
    "# 若改为真实训练，请按模型与显存调大 vocab/hidden/layer/head 等参数，并重新生成 tokenizer/权重。\n",
    "vocab_size = 10   # 词表大小，此处用极小词表便于演示\n",
    "hidden_size = 128  # GPT2 隐藏维度/embedding 维度\n",
    "intermediate_size = 256  # FFN 中间层维度，通常是 hidden 的 2-4 倍\n",
    "num_hidden_layers = 2  # Transformer block 层数（教学例子用 2 层）\n",
    "num_attention_heads = 4  # 多头注意力的头数\n",
    "batch_size = 3  # 演示批大小\n",
    "length_x = 5  # prompt 长度（输入序列长度）\n",
    "max_new_tokens = 5  # 生成时最多追加的 token 数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化actor模型\n",
    "\n",
    "以GPT2为例，初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yxc/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "# 固定随机种子，保证多次运行的参数初始化和示例输出一致，便于教学复现\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# ---- 定义 GPT2 配置（演示用小模型） ----\n",
    "# 下方参数会覆盖前面的超参，确保 config 与后续模型/分词器兼容；这里依然使用小模型以降低计算量。\n",
    "vocab_size = 10\n",
    "hidden_size = 128\n",
    "intermediate_size = 256\n",
    "num_hidden_layers = 2\n",
    "num_attention_heads = 4\n",
    "\n",
    "# 构造 GPT2Config：主要控制词表大小、隐藏维度、层数、注意力头数\n",
    "# 注意 vocab_size 仍设为官方 GPT-2 词表大小 50257，便于直接复用预训练 tokenizer；\n",
    "# 如需完全自定义小词表，需要同时训练或加载对应 tokenizer。\n",
    "config = GPT2Config(\n",
    "    vocab_size=50257,          # 这里用标准 GPT-2 词表大小，方便加载 tokenizer\n",
    "    n_embd=hidden_size,        # hidden/embedding 维度\n",
    "    n_inner=intermediate_size, # FFN 中间层维度\n",
    "    n_layer=num_hidden_layers, # Transformer block 层数\n",
    "    n_head=num_attention_heads # 注意力头数\n",
    ")\n",
    "\n",
    "# 基于配置实例化一个 GPT-2 语言模型（带 LM 头）；此处不会加载预训练权重，完全随机初始化\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model generate\n",
    "\n",
    "主要看下inputs_ids和attention_mask的含义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inputs_ids\n",
    "\n",
    "input_ids：它是一个张量（tensor），表示文本被分词后每个词（token）对应的 ID。比如在第一行 [20015, 232, 25465, ...] 中，每个数字都是原文本中一个词被 GPT - 2 分词器转换后的唯一标识。不同模型的词表不同，这些 ID 对应的具体词汇也不一样。这里第一行可能对应一句中文文本分词结果，第二行 [14150, 257, 922, ...] 前半部分对应英文文本，后半部分 50256 一般是填充值 ，表示补齐固定长度。\n",
    "\n",
    "\n",
    "attention_mask：同样是张量，用于指示哪些位置是有效的词（值为 1），哪些位置是填充的（值为 0） 。比如第二行 [1, 1, 1, 1, 0, 0, 0, 0, 0, 0] 表示前 4 个词是有效输入，后面是填充的，模型在处理时会忽略填充位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs_ids可以认为是要输入的文本经过tokenizer处理后的结果，而attention_mask则是用于指示哪些位置是有效的词（值为 1），哪些位置是填充的（值为 0） 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[20015,   232, 25465, 25465, 36365,   242, 38834,   165,   242,   247],\n",
      "        [14150,   257,   922,  1110, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# 初始化 GPT-2 分词器；使用官方权重以复用词表/分词规则\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# GPT-2 原生没有 pad_token，这里把 pad_token 设为 eos 方便 batch padding\n",
    "# 注意：将 pad 设为 eos 可能会让模型混淆结束符与填充位，后续需配合 attention_mask 使用\n",
    "# 如需严格区分，可自定义 pad_token 并重新训练/映射词表。\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 示例输入（中英文各一句）\n",
    "inputs = ['今天天气不错', 'have a good day']\n",
    "\n",
    "# 批量分词并右侧填充到同长，返回 PyTorch 张量\n",
    "# padding=True 触发右填充（默认为 right），truncation=True 避免超长序列\n",
    "inputs = tokenizer(\n",
    "    inputs,\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "print(inputs)  # 打印 input_ids 和 attention_mask，观察填充位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[20015,   232, 25465, 25465, 36365,   242, 38834,   165,   242,   247,\n",
      "           247,   247,   247,   247,   247],\n",
      "        [14150,   257,   922,  1110, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "# 未显式传入 attention_mask，会触发 warnings 并在 decode 端可能出现异常填充；演示保留，便于对比\n",
    "output_ids = model.generate(inputs['input_ids'], max_new_tokens=max_new_tokens)\n",
    "print(output_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今天天气不错�����', 'have a good day']\n"
     ]
    }
   ],
   "source": [
    "# 将生成的 token 序列转回文本，skip_special_tokens=True 以去除 eos/pad 等特殊符号\n",
    "output_ids = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "print(output_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "填充左边和右边会导致input_ids中padding_id的位置不一样，导致attention_mask中padding_id的位置不一样，导致模型在处理时会忽略填充位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[20015,   232, 25465, 25465, 36365,   242, 38834,   165,   242,   247],\n",
      "        [50256, 50256, 50256, 50256, 50256, 50256, 14150,   257,   922,  1110]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]])}\n",
      "tensor([[20015,   232, 25465, 25465, 36365,   242, 38834,   165,   242,   247,\n",
      "           247,   247,   247,   247,   247],\n",
      "        [50256, 50256, 50256, 50256, 50256, 50256, 14150,   257,   922,  1110,\n",
      "          1110,  1110,  1110,  1110,  1110]])\n",
      "['今天天气不错�����', 'have a good day day day day day day']\n"
     ]
    }
   ],
   "source": [
    "# 改为左填充，便于 decoder-only 结构推理时让有效 token 在序列尾部（更符合 GPT 的位置编码习惯）\n",
    "tokenizer.padding_side = 'left'\n",
    "inputs = ['今天天气不错', 'have a good day']\n",
    "inputs = tokenizer(inputs, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "print(inputs)\n",
    "\n",
    "# 仍未传入 attention_mask，此处只为对比左右填充对输出的影响\n",
    "output_ids = model.generate(inputs['input_ids'], max_new_tokens=max_new_tokens)\n",
    "\n",
    "print(output_ids)\n",
    "\n",
    "# 解码生成结果，观察因填充方式不同导致的重复等现象\n",
    "output_ids = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "print(output_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 现在开始正式讲rlhf流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化reward model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据之前的定义，奖励模型可以从模型的输出中提取出最后一个token的隐藏状态，然后通过一个线性层计算奖励。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设batch_size = 2, sequence_length = 4\n",
    "input_ids = torch.tensor([\n",
    "    [1, 2, 3, 4],  # 第一个序列\n",
    "    [5, 6, 7, 8]   # 第二个序列\n",
    "])\n",
    "\n",
    "attention_mask = torch.tensor([\n",
    "    [1, 1, 1, 0],  # 第一个序列有效长度为3\n",
    "    [1, 1, 1, 1]   # 第二个序列有效长度为4\n",
    "])\n",
    "\n",
    "sequence_length = attention_mask.sum(dim=1).long() - 1\n",
    "\n",
    "结果: tensor([2, 3])\n",
    "\n",
    "第一个序列：3-1=2（索引从0开始）\n",
    "\n",
    "第二个序列：4-1=3\n",
    "\n",
    "batch_indices = torch.arange(batch_size)\n",
    "\n",
    "结果: tensor([0, 1])\n",
    "\n",
    "假设hidden_size = 2\n",
    "\n",
    "last_hidden_state = torch.tensor([\n",
    "    [[1.0, 1.1], [2.0, 2.1], [3.0, 3.1], [4.0, 4.1]],  # 第一个序列\n",
    "    [[5.0, 5.1], [6.0, 6.1], [7.0, 7.1], [8.0, 8.1]]   # 第二个序列\n",
    "])\n",
    "\n",
    "使用batch_indices和sequence_length提取\n",
    "\n",
    "result = last_hidden_state[batch_indices, sequence_length]\n",
    "\n",
    "结果: tensor([[3.0, 3.1],    # 第一个序列的第2个位置（索引从0开始）\n",
    "\n",
    "[8.0, 8.1]])   # 第二个序列的第3个位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTRewardModel(torch.nn.Module):\n",
    "    def __init__(self, gpt_model, reward_head):\n",
    "        super(GPTRewardModel, self).__init__()\n",
    "        self.gpt_model = gpt_model  # 共享 backbone，保持与 actor 一致\n",
    "        self.reward_head = reward_head  # 将最后一个 token 的隐藏状态映射到标量奖励\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 获取模型的输出；需打开 output_hidden_states 才能拿到所有层的 hidden_states\n",
    "        outputs = self.gpt_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # 通常取最后一个隐藏状态作为输出（hidden_states[-1] 形状: [B, T, H]）\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        batch_size = input_ids.shape[0]\n",
    "        # 根据 attention_mask 计算每条序列的有效长度，减 1 得到最后一个有效 token 的索引\n",
    "        sequence_length = attention_mask.sum(dim=1).long() - 1\n",
    "        # 使用 batch 维索引选取每条序列的末 token 表征\n",
    "        batch_indices = torch.arange(batch_size, device=input_ids.device).long()\n",
    "        last_hidden_state = last_hidden_state[batch_indices, sequence_length]\n",
    "        print(f\"last_hidden_state shape: {last_hidden_state.shape}, sequence_length: {sequence_length.shape}\")\n",
    "        # 计算奖励（线性投影到标量）；输出形状 [B, 1]\n",
    "        rewards = self.reward_head(last_hidden_state)\n",
    "        return rewards\n",
    "\n",
    "# 重新初始化模型并开启 hidden_states 输出，便于 reward/value 头复用\n",
    "model.config.output_hidden_states = True\n",
    "rm_model = GPTRewardModel(model, torch.nn.Linear(hidden_size, 1))  # reward_head: H->1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20015,   232, 25465, 25465, 36365,   242, 38834,   165,   242,   247],\n",
       "        [50256, 50256, 50256, 50256, 50256, 50256, 14150,   257,   922,  1110]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state shape: torch.Size([2, 128]), sequence_length: torch.Size([2])\n",
      "tensor([[-0.1647],\n",
      "        [-0.2839]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 通过奖励头计算每条序列末 token 的奖励；输出形状 [batch, 1]\n",
    "reward = rm_model(inputs['input_ids'], inputs['attention_mask'])\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简化版ppo\n",
    "从以上过程可以看出，我们输入给模型的其实是input_ids和attention_mask，所以我们现在为了展示方便，构造一个没有实际意义的输入，输入给模型，然后输出奖励。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造模拟的 prompt/response 离散 token 序列，仅用于演示，无语义意义\n",
    "prompt = torch.randint(0, vocab_size, (batch_size, length_x))\n",
    "response = torch.randint(0, vocab_size, (batch_size, length_x + max_new_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 0, 0, 1, 0],\n",
      "        [4, 8, 1, 4, 1],\n",
      "        [9, 6, 7, 0, 5]])\n",
      "tensor([[4, 8, 5, 2, 9, 5, 5, 0, 6, 3],\n",
      "        [0, 3, 0, 4, 8, 2, 6, 4, 9, 3],\n",
      "        [2, 6, 7, 5, 0, 0, 3, 3, 4, 8]])\n"
     ]
    }
   ],
   "source": [
    "# 打印模拟的 prompt 与 response，便于后续对齐 mask 和 logprob\n",
    "print(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们希望让模型只关注response，所以对prompt对应的mask置为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 为 response 构造 mask：prompt 部分置 0，response 部分置 1，只在生成片段上计算 logprob/value/reward\n",
    "attention_mask = torch.ones(batch_size, length_x + max_new_tokens)\n",
    "attention_mask[:, :length_x] = 0\n",
    "print(attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt 的 attention_mask 全 1，若需要将 prompt 也参与 logprob 计算可复用\n",
    "prompt_attention_mask = torch.ones(batch_size, length_x)\n",
    "prompt_attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建几个模型\n",
    "\n",
    "\n",
    "model_ref 和model的配置一样\n",
    "\n",
    "reward model和value model的配置大体一样\n",
    "\n",
    "value model的输出是所有token的隐藏状态所得到的value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 初始化参考策略模型（ref）；与 actor 使用相同 config，保持权重基线用于 KL 惩罚\n",
    "model_ref = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 128)\n",
      "    (wpe): Embedding(1024, 128)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-1): 2 x GPT2Block(\n",
      "        (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D(nf=384, nx=128)\n",
      "          (c_proj): Conv1D(nf=128, nx=128)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=256, nx=128)\n",
      "          (c_proj): Conv1D(nf=128, nx=256)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=128, out_features=50257, bias=False)\n",
      ")\n",
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 128)\n",
      "    (wpe): Embedding(1024, 128)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-1): 2 x GPT2Block(\n",
      "        (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D(nf=384, nx=128)\n",
      "          (c_proj): Conv1D(nf=128, nx=128)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=256, nx=128)\n",
      "          (c_proj): Conv1D(nf=128, nx=256)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=128, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 对比 reference 与 actor 结构，确认两者一致便于后续 KL 计算\n",
    "print(model_ref)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化value model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们有以下维度的数据：\n",
    "\n",
    "last_hidden_state 的形状是 [batch_size, sequence_length, hidden_size]\n",
    "\n",
    "比如 [5, 10, 128]，表示批次大小为5，序列长度为10，隐藏层维度为128\n",
    "\n",
    "self.value_head 是一个线性层 Linear(hidden_size, 1)\n",
    "\n",
    "输入维度是128，输出维度是1\n",
    "\n",
    "处理过程：\n",
    "\n",
    "self.value_head(last_hidden_state) 的操作：\n",
    "\n",
    "输入: [5, 10, 128]\n",
    "\n",
    "输出: [5, 10, 1] # 线性层将最后一个维度从128转换为1\n",
    "\n",
    "[:, :, 0] 的操作：\n",
    "\n",
    "取最后一个维度的第0个元素\n",
    "\n",
    "结果形状变为: [5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTValueModel(torch.nn.Module):\n",
    "    def __init__(self, gpt_model, value_head):\n",
    "        super().__init__()\n",
    "        self.gpt_model = gpt_model  # 共享 backbone\n",
    "        self.value_head = value_head  # 线性层映射到标量 value\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 前向推理并取最后一层隐藏状态；形状 [B, T, H]\n",
    "        outputs = self.gpt_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        # value_head 输出形状 [B, T, 1]，再去掉最后一维得到 [B, T]\n",
    "        values = self.value_head(last_hidden_state)[:, :, 0]\n",
    "        return values\n",
    "    \n",
    "# 打开 hidden_states，便于 value_head 读取；重用 actor 权重做 critic（教学示例）\n",
    "model.config.output_hidden_states = True\n",
    "vm_model = GPTValueModel(model, torch.nn.Linear(hidden_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTRewardModel(\n",
      "  (gpt_model): GPT2LMHeadModel(\n",
      "    (transformer): GPT2Model(\n",
      "      (wte): Embedding(50257, 128)\n",
      "      (wpe): Embedding(1024, 128)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0-1): 2 x GPT2Block(\n",
      "          (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2SdpaAttention(\n",
      "            (c_attn): Conv1D(nf=384, nx=128)\n",
      "            (c_proj): Conv1D(nf=128, nx=128)\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D(nf=256, nx=128)\n",
      "            (c_proj): Conv1D(nf=128, nx=256)\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=128, out_features=50257, bias=False)\n",
      "  )\n",
      "  (reward_head): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "GPTValueModel(\n",
      "  (gpt_model): GPT2LMHeadModel(\n",
      "    (transformer): GPT2Model(\n",
      "      (wte): Embedding(50257, 128)\n",
      "      (wpe): Embedding(1024, 128)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0-1): 2 x GPT2Block(\n",
      "          (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2SdpaAttention(\n",
      "            (c_attn): Conv1D(nf=384, nx=128)\n",
      "            (c_proj): Conv1D(nf=128, nx=128)\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D(nf=256, nx=128)\n",
      "            (c_proj): Conv1D(nf=128, nx=256)\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=128, out_features=50257, bias=False)\n",
      "  )\n",
      "  (value_head): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 打印 reward/value 模型，确认头部与 backbone 绑定\n",
    "print(rm_model)\n",
    "print(vm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ppo前向过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建几个model的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(model, prompt, max_new_tokens, attention_mask):\n",
    "    # 调用 generate 生成序列；教学示例传入 attention_mask 以避免 pad 干扰\n",
    "    inputs = {'input_ids': prompt, 'attention_mask': attention_mask}\n",
    "    y = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        # forced_eos_token_id=True  # 如需强制结束可打开\n",
    "    )\n",
    "    return y\n",
    "\n",
    "\n",
    "def get_reward(model, response, attention_mask):\n",
    "    # 计算 reward 模型输出；reward_head 仅使用最后一个有效 token\n",
    "    inputs = {'input_ids': response, 'attention_mask': attention_mask}\n",
    "    y = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "    return y\n",
    "\n",
    "\n",
    "def get_value(model, prompt, attention_mask):\n",
    "    # 计算 value 序列，每个 token 都输出一个 value，形状 [B, T]\n",
    "    inputs = {'input_ids': prompt, 'attention_mask': attention_mask}\n",
    "    y = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 0, 0, 1, 0],\n",
       "        [4, 8, 1, 4, 1],\n",
       "        [9, 6, 7, 0, 5]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看模拟的 prompt 张量\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 8, 5, 2, 9, 5, 5, 0, 6, 3],\n",
       "        [0, 3, 0, 4, 8, 2, 6, 4, 9, 3],\n",
       "        [2, 6, 7, 5, 0, 0, 3, 3, 4, 8]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看模拟的 response 张量\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看 prompt 的 mask（全 1）\n",
    "prompt_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看 response 部分的 mask（前半 0，后半 1）\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里就可以看到，ppo流程中的reward只是在最后一个token上得到的，但是我的value model要在每一个token上得到一个价值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    5,     0,     0,     1,     0,     0,     0,     0,     0,     0],\n",
      "        [    4,     8,     1,     4,     1, 10998, 10998, 10998, 10998, 10998],\n",
      "        [    9,     6,     7,     0,     5,     5,     5,     5,     5,     5]])\n",
      "last_hidden_state shape: torch.Size([3, 128]), sequence_length: torch.Size([3])\n",
      "tensor([[-0.4702],\n",
      "        [-1.0223],\n",
      "        [-0.6396]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.1054, -0.1810, -0.2179, -0.4633, -0.1662,  0.0374, -0.7071, -0.7640,\n",
      "         -1.3427,  0.2779],\n",
      "        [ 0.0424, -0.0425, -1.1631, -0.1351,  0.2049,  0.0207, -0.9090,  0.4028,\n",
      "         -0.1427,  0.6911],\n",
      "        [ 0.1912, -0.2840,  0.1110,  0.6809, -0.4596, -0.1590, -0.2637, -0.3191,\n",
      "         -0.1446,  0.9440]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 依次展示：生成的响应、末 token 奖励、逐 token value（仅示例，不做训练）\n",
    "print(get_response(model, prompt, max_new_tokens, prompt_attention_mask))\n",
    "print(get_reward(rm_model, response, attention_mask))\n",
    "print(get_value(vm_model, response, attention_mask))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO 相关设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "封装几个ppo的model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOModels():\n",
    "    def __init__(self, model_actor, model_ref, model_rm, model_critic):\n",
    "        # 统一封装 actor/ref/reward/critic，便于传递给训练函数\n",
    "        self.actor = model_actor\n",
    "        self.ref = model_ref\n",
    "        self.rm = model_rm\n",
    "        self.critic = model_critic\n",
    "\n",
    "# 参考模型与奖励模型在训练中保持冻结（eval），避免梯度更新\n",
    "model_ref.eval()\n",
    "rm_model.eval()\n",
    "models = PPOModels(model, model_ref, rm_model, vm_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置ppo的超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ppo_epochs在每次策略更新时，PPO 算法对收集到的数据进行迭代训练的次数。\n",
    "\n",
    "2. mini_batch_size每个训练步骤中，从收集到的数据里选取的小批量数据的样本数量。\n",
    "\n",
    "3. epochs整个训练过程中，算法对所有收集到的数据进行完整遍历的次数。\n",
    "\n",
    "4. kl_ctlKL 散度惩罚项的系数，用于控制新旧策略之间的差异程度。\n",
    "\n",
    "5. vf_coef价值函数损失的系数，用于平衡策略损失和价值函数损失在总损失中的权重。\n",
    "\n",
    "6. lam广义优势估计（GAE）中的 \\(\\lambda\\) 参数，用于平衡优势估计的偏差和方差。\n",
    "\n",
    "7. gamma折扣因子，用于计算未来奖励的折现值，决定未来奖励在当前价值估计中的重要程度。\n",
    "\n",
    "8. cliprange_value价值函数裁剪范围的参数，用于限制价值函数更新的幅度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOConfig():\n",
    "    def __init__(self):\n",
    "        # ppo_epochs: 每轮 update 中重复遍历批次的次数\n",
    "        self.ppo_epochs = 5\n",
    "        # mini_batch_size: 从采样的 batch 中切分出的子批大小\n",
    "        self.mini_batch_size = 2\n",
    "        # epochs: 数据集/采样整体遍历轮数（此处未实现完整数据循环，仅示意）\n",
    "        self.epochs = 4\n",
    "        # kl_ctl: KL 惩罚系数，约束 actor 偏离 ref 的幅度\n",
    "        self.kl_ctl = 0.1\n",
    "        # vf_coef: value loss 权重\n",
    "        self.vf_coef = 0.1\n",
    "        # lam: GAE 衰减因子\n",
    "        self.lam = 0.9\n",
    "        # gamma: 折扣因子\n",
    "        self.gamma = 0.9\n",
    "        # cliprange_value: value 裁剪范围\n",
    "        self.cliprange_value = 0.2\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'ppo_epochs:{self.ppo_epochs}\\nmini_batch_size:{self.mini_batch_size}\\nepochs:{self.epochs}\\nkl_ctl:{self.kl_ctl}'\n",
    "\n",
    "\n",
    "ppo_config = PPOConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在每一步中ppo都在干什么"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先要有个列表来记录每一步的采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用字典暂存一次采样得到的各类张量，便于后续计算损失和更新\n",
    "ppo_old_batchs = {\n",
    "    'prompt': None,\n",
    "    'response': None,\n",
    "    'mask': None,\n",
    "    'logprobs_ref': None,\n",
    "    'logprobs_old': None,\n",
    "    'logprobs': None,\n",
    "    'values_old': None,\n",
    "    'values': None,\n",
    "    'rewards': None,\n",
    "    'rewards_kl': None,\n",
    "    'loss': None,\n",
    "    'logits': None,\n",
    "}\n",
    "\n",
    "ppo_old_batchs['prompt'] = prompt\n",
    "ppo_old_batchs['response'] = response\n",
    "ppo_old_batchs['mask'] = attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': tensor([[5, 0, 0, 1, 0],\n",
       "         [4, 8, 1, 4, 1],\n",
       "         [9, 6, 7, 0, 5]]),\n",
       " 'response': tensor([[4, 8, 5, 2, 9, 5, 5, 0, 6, 3],\n",
       "         [0, 3, 0, 4, 8, 2, 6, 4, 9, 3],\n",
       "         [2, 6, 7, 5, 0, 0, 3, 3, 4, 8]]),\n",
       " 'mask': tensor([[0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]]),\n",
       " 'logprobs_ref': None,\n",
       " 'logprobs_old': None,\n",
       " 'logprobs': None,\n",
       " 'values_old': None,\n",
       " 'values': None,\n",
       " 'rewards': None,\n",
       " 'rewards_kl': None,\n",
       " 'loss': None,\n",
       " 'logits': None}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_old_batchs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前向推理，得到token的logprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logprobs = F.log_softmax(logits, dim=-1)第一步:对logits进行softmax并取log\n",
    "\n",
    "torch.gather是一个用于从张量中按索引收集值的操作 \n",
    "\n",
    "假设我们有:\n",
    "\n",
    "logp.shape = [1, 5, 32]      # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "labels.shape = [1, 5]        # [batch_size, seq_len]\n",
    "\n",
    "1. labels.unsqueeze(2)\n",
    "\n",
    "在最后增加一个维度\n",
    "\n",
    "labels_expanded = labels.unsqueeze(2)   # shape变为[1, 5, 1]\n",
    "\n",
    "2. torch.gather(logp, 2, labels_expanded)\n",
    "\n",
    "dim=2表示在词表维度(第3维)上收集值\n",
    "\n",
    "gathered = torch.gather(logp, 2, labels_expanded)  # shape为[1, 5, 1]\n",
    "\n",
    "3. squeeze(-1)\n",
    "\n",
    "去掉最后一个维度\n",
    "\n",
    "logpy = gathered.squeeze(-1)  # 最终shape为[1, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_ids shape: torch.Size([3, 10])\n",
      "logits shape: torch.Size([3, 10, 50257])\n",
      "logits shape: torch.Size([3, 10, 50257]), response shape: torch.Size([3, 10]), attention_mask shape: torch.Size([3, 10])\n",
      "all_token_logprobs shape: torch.Size([3, 10, 50257])\n",
      "gathered shape: torch.Size([3, 10, 1]), response shape: torch.Size([3, 10])\n",
      "response_logprobs shape: torch.Size([3, 10])\n",
      "\n",
      "\n",
      "inputs_ids shape: torch.Size([3, 10])\n",
      "logits shape: torch.Size([3, 10, 50257])\n",
      "logits shape: torch.Size([3, 10, 50257]), response shape: torch.Size([3, 10]), attention_mask shape: torch.Size([3, 10])\n",
      "all_token_logprobs shape: torch.Size([3, 10, 50257])\n",
      "gathered shape: torch.Size([3, 10, 1]), response shape: torch.Size([3, 10])\n",
      "response_logprobs shape: torch.Size([3, 10])\n",
      "\n",
      "\n",
      "inputs_ids shape: torch.Size([3, 10])\n",
      "logits shape: torch.Size([3, 10, 50257])\n",
      "logits shape: torch.Size([3, 10, 50257]), response shape: torch.Size([3, 10]), attention_mask shape: torch.Size([3, 10])\n",
      "all_token_logprobs shape: torch.Size([3, 10, 50257])\n",
      "gathered shape: torch.Size([3, 10, 1]), response shape: torch.Size([3, 10])\n",
      "response_logprobs shape: torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_logits(model, input_ids):\n",
    "    # 前向得到 logits，形状 [B, T, Vocab]；未传 mask，这里仅演示收集响应 token 的对数概率\n",
    "    outputs = model(input_ids=input_ids)\n",
    "    print(f\"inputs_ids shape: {input_ids.shape}\")\n",
    "    logits = outputs.logits\n",
    "    print(f\"logits shape: {logits.shape}\")\n",
    "    return logits\n",
    "\n",
    "def get_logprobs(model, response, attention_mask):\n",
    "    # 计算指定 response 的逐 token 对数概率\n",
    "    logits = get_logits(model, response)\n",
    "    print(f\"logits shape: {logits.shape}, response shape: {response.shape}, attention_mask shape: {attention_mask.shape}\")\n",
    "    # 先 softmax 再取对数，得到所有 token 的 logprob\n",
    "    all_token_logprobs = F.log_softmax(logits, dim=-1)\n",
    "    print(f\"all_token_logprobs shape: {all_token_logprobs.shape}\")\n",
    "    # 根据 response 的 token id 收集对应 logprob；attention_mask 未用于 gather，仅打印形状\n",
    "    gathered = torch.gather(all_token_logprobs, 2, response.unsqueeze(2))\n",
    "    print(f\"gathered shape: {gathered.shape}, response shape: {response.shape}\")\n",
    "    # 去掉最后一维，得到 [B, T]\n",
    "    response_logprobs = gathered.squeeze(-1)\n",
    "    print(f\"response_logprobs shape: {response_logprobs.shape}\")\n",
    "    return response_logprobs\n",
    "\n",
    "# 参考模型 logprob（计算 KL 基线）\n",
    "logprobs_ref = get_logprobs(models.ref, ppo_old_batchs['response'], ppo_old_batchs['mask'])\n",
    "print('\\n')\n",
    "# 旧策略（当前 actor）logprob，用于 PPO ratio\n",
    "logprobs_old = get_logprobs(models.actor, ppo_old_batchs['response'], ppo_old_batchs['mask'])\n",
    "print('\\n')\n",
    "# 重新计算当前 logprob（示例中与 old 相同，因为未更新参数）\n",
    "logprobs = get_logprobs(models.actor, ppo_old_batchs['response'], ppo_old_batchs['mask'])\n",
    "\n",
    "print(logprobs_ref.shape)\n",
    "print(logprobs_old.shape)\n",
    "print(logprobs.shape)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -9.6364, -10.0382,  -9.4454,  -9.7810,  -9.3484,  -9.5437,  -9.6146,\n",
       "          -9.3174,  -9.8408,  -9.5032],\n",
       "        [ -9.6546,  -9.7166,  -9.7343,  -9.4578,  -9.8507,  -9.7604,  -9.8515,\n",
       "          -9.6053,  -9.3741,  -9.4720],\n",
       "        [ -9.8447, -10.2057,  -9.4921,  -9.7237,  -9.1873,  -9.4923,  -9.6284,\n",
       "          -9.9353,  -9.3172,  -9.8445]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0130,  0.0095, -0.0262, -0.0021, -0.0283, -0.0148, -0.0134, -0.0258,\n",
      "          0.0089, -0.0307],\n",
      "        [-0.0315, -0.0049, -0.0047, -0.0323,  0.0020, -0.0178,  0.0170, -0.0316,\n",
      "         -0.0339, -0.0369],\n",
      "        [-0.0574,  0.0419, -0.0651, -0.0085, -0.0412, -0.0019, -0.0238,  0.0211,\n",
      "         -0.0333,  0.0152]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def get_kl(logprobs_ref, logprobs_old, kl_ctl):\n",
    "    # 逐 token 近似 KL：log p_ref - log p_actor，并乘以系数作为惩罚\n",
    "    kl = logprobs_ref - logprobs_old\n",
    "    kl = kl * kl_ctl\n",
    "    return kl\n",
    "\n",
    "# 计算 KL 惩罚（未加总），用于 reward 调整\n",
    "kl = get_kl(logprobs_ref, logprobs_old, ppo_config.kl_ctl)\n",
    "print(kl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算reward_kl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0130,  0.0095, -0.0262, -0.0021, -0.0283, -0.0148, -0.0134, -0.0258,\n",
      "          0.0089, -0.0307],\n",
      "        [-0.0315, -0.0049, -0.0047, -0.0323,  0.0020, -0.0178,  0.0170, -0.0316,\n",
      "         -0.0339, -0.0369],\n",
      "        [-0.0574,  0.0419, -0.0651, -0.0085, -0.0412, -0.0019, -0.0238,  0.0211,\n",
      "         -0.0333,  0.0152]], grad_fn=<MulBackward0>)\n",
      "last_hidden_state shape: torch.Size([3, 128]), sequence_length: torch.Size([3])\n",
      "tensor([[-0.7784],\n",
      "        [-0.9515],\n",
      "        [-0.9003]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0130,  0.0095, -0.0262, -0.0021, -0.0283, -0.0148, -0.0134, -0.0258,\n",
      "          0.0089, -0.8090],\n",
      "        [-0.0315, -0.0049, -0.0047, -0.0323,  0.0020, -0.0178,  0.0170, -0.0316,\n",
      "         -0.0339, -0.9884],\n",
      "        [-0.0574,  0.0419, -0.0651, -0.0085, -0.0412, -0.0019, -0.0238,  0.0211,\n",
      "         -0.0333, -0.8852]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "def get_reward_with_kl(logprobs_ref, logprobs_old, kl_ctl, reward):\n",
    "    # KL 惩罚加到逐 token 奖励上；末 token 额外叠加 RM 打分，形成 r + KL 约束\n",
    "    kl = logprobs_ref - logprobs_old\n",
    "    kl = kl * kl_ctl\n",
    "    kl[:, -1] += reward[:, 0]\n",
    "    return kl\n",
    "\n",
    "print(kl)\n",
    "# 计算奖励模型输出（末 token），随后与 KL 惩罚合并\n",
    "rewards = get_reward(models.rm, ppo_old_batchs['response'], ppo_old_batchs['mask'])\n",
    "print(rewards)\n",
    "\n",
    "kl_reward = get_reward_with_kl(logprobs_ref, logprobs_old, ppo_config.kl_ctl, rewards)\n",
    "print(kl_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算 value 序列（与 actor 共享 backbone 的简化示例）\n",
    "values = get_value(models.critic, ppo_old_batchs['response'], ppo_old_batchs['mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1939, -0.0731, -0.0170, -0.4315,  0.0534, -0.2046, -0.6074, -0.7700,\n",
       "         -1.2505,  0.1553],\n",
       "        [ 0.0511, -0.2098, -0.8512, -0.1117,  0.2560, -0.0967, -0.9718,  0.2660,\n",
       "         -0.1777,  0.4735],\n",
       "        [ 0.2042, -0.6096, -0.0284,  0.2577, -0.3757, -0.3134, -0.5433, -0.2487,\n",
       "         -0.2369,  1.0747]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': tensor([[5, 0, 0, 1, 0],\n",
       "         [4, 8, 1, 4, 1],\n",
       "         [9, 6, 7, 0, 5]]),\n",
       " 'response': tensor([[4, 8, 5, 2, 9, 5, 5, 0, 6, 3],\n",
       "         [0, 3, 0, 4, 8, 2, 6, 4, 9, 3],\n",
       "         [2, 6, 7, 5, 0, 0, 3, 3, 4, 8]]),\n",
       " 'mask': tensor([[0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]]),\n",
       " 'logprobs_ref': tensor([[ -9.7659,  -9.9431,  -9.7075,  -9.8018,  -9.6310,  -9.6916,  -9.7483,\n",
       "           -9.5755,  -9.7520,  -9.8097],\n",
       "         [ -9.9691,  -9.7657,  -9.7810,  -9.7806,  -9.8304,  -9.9382,  -9.6816,\n",
       "           -9.9212,  -9.7132,  -9.8413],\n",
       "         [-10.4189,  -9.7863, -10.1431,  -9.8084,  -9.5995,  -9.5113,  -9.8666,\n",
       "           -9.7238,  -9.6501,  -9.6926]], grad_fn=<SqueezeBackward1>),\n",
       " 'logprobs_old': tensor([[ -9.6364, -10.0382,  -9.4454,  -9.7810,  -9.3484,  -9.5437,  -9.6146,\n",
       "           -9.3174,  -9.8408,  -9.5032],\n",
       "         [ -9.6546,  -9.7166,  -9.7343,  -9.4578,  -9.8507,  -9.7604,  -9.8515,\n",
       "           -9.6053,  -9.3741,  -9.4720],\n",
       "         [ -9.8447, -10.2057,  -9.4921,  -9.7237,  -9.1873,  -9.4923,  -9.6284,\n",
       "           -9.9353,  -9.3172,  -9.8445]], grad_fn=<SqueezeBackward1>),\n",
       " 'logprobs': tensor([[ -9.6364, -10.0382,  -9.4454,  -9.7810,  -9.3484,  -9.5437,  -9.6146,\n",
       "           -9.3174,  -9.8408,  -9.5032],\n",
       "         [ -9.6546,  -9.7166,  -9.7343,  -9.4578,  -9.8507,  -9.7604,  -9.8515,\n",
       "           -9.6053,  -9.3741,  -9.4720],\n",
       "         [ -9.8447, -10.2057,  -9.4921,  -9.7237,  -9.1873,  -9.4923,  -9.6284,\n",
       "           -9.9353,  -9.3172,  -9.8445]], grad_fn=<SqueezeBackward1>),\n",
       " 'values_old': tensor([[ 0.1939, -0.0731, -0.0170, -0.4315,  0.0534, -0.2046, -0.6074, -0.7700,\n",
       "          -1.2505,  0.1553],\n",
       "         [ 0.0511, -0.2098, -0.8512, -0.1117,  0.2560, -0.0967, -0.9718,  0.2660,\n",
       "          -0.1777,  0.4735],\n",
       "         [ 0.2042, -0.6096, -0.0284,  0.2577, -0.3757, -0.3134, -0.5433, -0.2487,\n",
       "          -0.2369,  1.0747]], grad_fn=<SelectBackward0>),\n",
       " 'values': None,\n",
       " 'rewards': tensor([[-0.7784],\n",
       "         [-0.9515],\n",
       "         [-0.9003]], grad_fn=<AddmmBackward0>),\n",
       " 'rewards_kl': tensor([[-0.0130,  0.0095, -0.0262, -0.0021, -0.0283, -0.0148, -0.0134, -0.0258,\n",
       "           0.0089, -0.8090],\n",
       "         [-0.0315, -0.0049, -0.0047, -0.0323,  0.0020, -0.0178,  0.0170, -0.0316,\n",
       "          -0.0339, -0.9884],\n",
       "         [-0.0574,  0.0419, -0.0651, -0.0085, -0.0412, -0.0019, -0.0238,  0.0211,\n",
       "          -0.0333, -0.8852]], grad_fn=<CopySlices>),\n",
       " 'loss': None,\n",
       " 'logits': None}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将当前计算得到的张量缓存入 batch 字典，便于后续 loss 计算\n",
    "ppo_old_batchs['logprobs_ref'] = logprobs_ref\n",
    "ppo_old_batchs['logprobs_old'] = logprobs_old\n",
    "ppo_old_batchs['logprobs'] = logprobs\n",
    "ppo_old_batchs['values_old'] = values\n",
    "ppo_old_batchs['rewards'] = rewards\n",
    "ppo_old_batchs['rewards_kl'] = kl_reward\n",
    "\n",
    "ppo_old_batchs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rewards：一个张量，代表在每个时间步获得的奖励。\n",
    "\n",
    "mask：一个掩码张量，用于标识哪些时间步是有效的（例如，用于处理终止状态）。\n",
    "\n",
    "values：一个张量，代表每个时间步的状态价值估计。\n",
    "\n",
    "gamma：折扣因子，用于计算未来奖励的折现值，取值范围通常在 [0, 1] 之间。\n",
    "\n",
    "lam：GAE 中的 \\(\\lambda\\) 参数，用于平衡偏差和方差，取值范围同样在 [0, 1] 之间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO 中的 GAE 公式\n",
    "\n",
    "在PPO（Proximal Policy Optimization）算法中，优势函数和价值损失是连接价值估计与策略优化的核心组件。\n",
    "\n",
    "## 优势函数（Advantage Function）\n",
    "\n",
    "优势函数衡量在某一状态下采取特定动作的**相对价值**，定义为：\n",
    "\n",
    "$$A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$$\n",
    "\n",
    "状态 - 动作价值函数（Q 函数），表示在状态 \\(s_t\\) 采取动作 \\(a_t\\) 后，从后续轨迹中获得的总折扣回报的期望。\n",
    "\n",
    "状态价值函数（V 函数），表示在状态 \\(s_t\\) 下，遵循当前策略时获得的总折扣回报的期望（即 “平均收益”）。\n",
    "\n",
    "优势函数的本质是回答：\n",
    "\n",
    "在状态 \\(s_t\\) 下选择动作 \\(a_t\\)，比‘按当前策略随机选一个动作’好多少？”\n",
    "\n",
    "若 \\(A(s_t, a_t) > 0\\)：动作 \\(a_t\\) 优于平均水平，值得鼓励（策略应提高该动作的概率）\n",
    "\n",
    "若 \\(A(s_t, a_t) < 0\\)：动作 \\(a_t\\) 劣于平均水平，应抑制（策略应降低该动作的概率）。\n",
    "\n",
    "优势函数将 “绝对价值” 转化为 “相对价值”，减少了估计偏差（例如，即使 \\(Q(s_t, a_t)\\) 和 \\(V(s_t)\\) 都有误差，两者的差值可能更稳定）\n",
    "\n",
    "在实际训练中，Q 和 V 无法直接获得，PPO 通常使用GAE（Generalized Advantage Estimation） 来估计优势函数\n",
    "\n",
    "GAE（Generalized Advantage Estimation）的时序差分残差公式：\n",
    "\n",
    "$$\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "\n",
    "其中，$r_t$ 是时间步 $t$ 的奖励，$\\gamma$ 是折扣因子，$V(s_t)$ 是状态 $s_t$ 的价值估计。\n",
    "\n",
    "GAE 优势估计的递归形式：\n",
    "\n",
    "$$\\hat{A}_t = \\delta_t + \\gamma \\lambda \\hat{A}_{t+1}$$\n",
    "\n",
    "其中 $\\lambda$ 是 GAE 的衰减参数（$0 \\leq \\lambda \\leq 1$）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GAE(rewards, attention_mask, values, gemma, lam):\n",
    "    # 基于 GAE 计算优势：先做时间反向累积，再翻转回来；支持掩码对齐有效 token\n",
    "    lastgae = 0  # 初始化为 0，用于存储上一个时间步的广义优势估计值。\n",
    "    advantages_recersed = []\n",
    "    response_len = rewards.shape[-1]\n",
    "\n",
    "    # 仅在有效 token 上计算，mask 处置零\n",
    "    values = values * attention_mask\n",
    "    rewards = rewards * attention_mask\n",
    "\n",
    "    for t in reversed(range(response_len)):\n",
    "        nextvalues = values[:, t + 1] if t < response_len - 1 else 0.0\n",
    "        # TD 误差：r_t + gamma * V_{t+1} - V_t\n",
    "        delta = rewards[:, t] + gemma * nextvalues - values[:, t]\n",
    "        # GAE 递推：A_t = delta + gamma * lambda * A_{t+1}\n",
    "        lastgae = delta + gemma * lam * lastgae\n",
    "        advantages_recersed.append(lastgae)\n",
    "    # 反转回正序，得到 [B, T]\n",
    "    advantages = torch.stack(advantages_recersed[::-1]).transpose(0, 1)\n",
    "    return advantages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': tensor([[5, 0, 0, 1, 0],\n",
       "         [4, 8, 1, 4, 1],\n",
       "         [9, 6, 7, 0, 5]]),\n",
       " 'response': tensor([[4, 8, 5, 2, 9, 5, 5, 0, 6, 3],\n",
       "         [0, 3, 0, 4, 8, 2, 6, 4, 9, 3],\n",
       "         [2, 6, 7, 5, 0, 0, 3, 3, 4, 8]]),\n",
       " 'mask': tensor([[0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]]),\n",
       " 'logprobs_ref': tensor([[ -9.7659,  -9.9431,  -9.7075,  -9.8018,  -9.6310,  -9.6916,  -9.7483,\n",
       "           -9.5755,  -9.7520,  -9.8097],\n",
       "         [ -9.9691,  -9.7657,  -9.7810,  -9.7806,  -9.8304,  -9.9382,  -9.6816,\n",
       "           -9.9212,  -9.7132,  -9.8413],\n",
       "         [-10.4189,  -9.7863, -10.1431,  -9.8084,  -9.5995,  -9.5113,  -9.8666,\n",
       "           -9.7238,  -9.6501,  -9.6926]], grad_fn=<SqueezeBackward1>),\n",
       " 'logprobs_old': tensor([[ -9.6364, -10.0382,  -9.4454,  -9.7810,  -9.3484,  -9.5437,  -9.6146,\n",
       "           -9.3174,  -9.8408,  -9.5032],\n",
       "         [ -9.6546,  -9.7166,  -9.7343,  -9.4578,  -9.8507,  -9.7604,  -9.8515,\n",
       "           -9.6053,  -9.3741,  -9.4720],\n",
       "         [ -9.8447, -10.2057,  -9.4921,  -9.7237,  -9.1873,  -9.4923,  -9.6284,\n",
       "           -9.9353,  -9.3172,  -9.8445]], grad_fn=<SqueezeBackward1>),\n",
       " 'logprobs': tensor([[ -9.6364, -10.0382,  -9.4454,  -9.7810,  -9.3484,  -9.5437,  -9.6146,\n",
       "           -9.3174,  -9.8408,  -9.5032],\n",
       "         [ -9.6546,  -9.7166,  -9.7343,  -9.4578,  -9.8507,  -9.7604,  -9.8515,\n",
       "           -9.6053,  -9.3741,  -9.4720],\n",
       "         [ -9.8447, -10.2057,  -9.4921,  -9.7237,  -9.1873,  -9.4923,  -9.6284,\n",
       "           -9.9353,  -9.3172,  -9.8445]], grad_fn=<SqueezeBackward1>),\n",
       " 'values_old': tensor([[ 0.1939, -0.0731, -0.0170, -0.4315,  0.0534, -0.2046, -0.6074, -0.7700,\n",
       "          -1.2505,  0.1553],\n",
       "         [ 0.0511, -0.2098, -0.8512, -0.1117,  0.2560, -0.0967, -0.9718,  0.2660,\n",
       "          -0.1777,  0.4735],\n",
       "         [ 0.2042, -0.6096, -0.0284,  0.2577, -0.3757, -0.3134, -0.5433, -0.2487,\n",
       "          -0.2369,  1.0747]], grad_fn=<SelectBackward0>),\n",
       " 'values': None,\n",
       " 'rewards': tensor([[-0.7784],\n",
       "         [-0.9515],\n",
       "         [-0.9003]], grad_fn=<AddmmBackward0>),\n",
       " 'rewards_kl': tensor([[-0.0130,  0.0095, -0.0262, -0.0021, -0.0283, -0.0148, -0.0134, -0.0258,\n",
       "           0.0089, -0.8090],\n",
       "         [-0.0315, -0.0049, -0.0047, -0.0323,  0.0020, -0.0178,  0.0170, -0.0316,\n",
       "          -0.0339, -0.9884],\n",
       "         [-0.0574,  0.0419, -0.0651, -0.0085, -0.0412, -0.0019, -0.0238,  0.0211,\n",
       "          -0.0333, -0.8852]], grad_fn=<CopySlices>),\n",
       " 'loss': None,\n",
       " 'logits': None}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_old_batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2043, -0.2523, -0.3115, -0.3845, -0.4747, -0.3587, -0.0023,  0.1193,\n",
       "          0.6180, -0.9643],\n",
       "        [-0.1865, -0.2303, -0.2843, -0.3509, -0.4333, -0.4275,  0.4546, -0.9550,\n",
       "         -0.6142, -1.4619],\n",
       "        [-0.1640, -0.2025, -0.2500, -0.3087, -0.3811, -0.1223,  0.0682, -0.2809,\n",
       "         -0.4166, -1.9599]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算优势：使用 KL 调整后的奖励 + 旧 value，得到每个 token 的 A_t\n",
    "gae = get_GAE(ppo_old_batchs['rewards_kl'], ppo_old_batchs['mask'], ppo_old_batchs['values_old'], ppo_config.gamma, ppo_config.lam)\n",
    "gae\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算value loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "advantages：优势函数的估计值，用于计算回报。\n",
    "\n",
    "\n",
    "values：当前价值函数的估计值。\n",
    "\n",
    "values_old：旧的价值函数估计值。\n",
    "\n",
    "mask：掩码张量，用于指定哪些元素参与损失计算。\n",
    "\n",
    "cliprange_value：裁剪范围，用于限制价值函数的更新幅度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/trl/blob/26d86757a7c7e24e397ea44f57ecce6031dfac01/trl/trainer/ppo_trainer.py#L561C29-L567C30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mean(values: torch.Tensor, mask: torch.Tensor, axis = None) -> torch.Tensor:\n",
    "    \"\"\"Compute mean of tensor with a masked values.\"\"\"\n",
    "    if axis is not None:\n",
    "        return (values * mask).sum(axis=axis) / mask.sum(axis=axis)\n",
    "    else:\n",
    "        return (values * mask).sum() / mask.sum()\n",
    "\n",
    "\n",
    "def get_value_loss(advantages, values, values_old, attention_mask, cliprange_value):\n",
    "    # 目标回报 = 旧价值估计 + 优势估计；returns 对齐 PPO 公式中的 V_old + A\n",
    "    returns = values_old + advantages\n",
    "    advantages = advantages.detach()\n",
    "    # 对新的价值估计 values 进行裁剪，限制其与旧价值估计 values_old 的差异不超过 cliprange_value\n",
    "    vpredclipped = torch.clamp(values, values_old - cliprange_value, values_old + cliprange_value)\n",
    "\n",
    "    # 两种误差：裁剪版与原版，取较大者以实现价值函数的 PPO-style 裁剪\n",
    "    vf_losses1 = torch.square(vpredclipped - returns)  # 裁剪后的价值估计与目标回报的平方误差\n",
    "    vf_losses2 = torch.square(values - returns)        # 未裁剪的价值估计与目标回报的平方误差\n",
    "    vf_loss_max = torch.max(vf_losses1, vf_losses2)\n",
    "    vf_loss = 0.5 * masked_mean(vf_loss_max, attention_mask)\n",
    "    return vf_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简化：将新 value 视为旧 value 加常数偏移，便于演示 value loss 裁剪\n",
    "ppo_old_batchs['values'] = ppo_old_batchs['values_old'] + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6554, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算 value 损失（带裁剪）\n",
    "value_loss = get_value_loss(gae, ppo_old_batchs['values'], ppo_old_batchs['values_old'], ppo_old_batchs['mask'], ppo_config.cliprange_value)\n",
    "value_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算policy loss\n",
    "https://github.com/huggingface/trl/blob/26d86757a7c7e24e397ea44f57ecce6031dfac01/trl/trainer/ppo_trainer.py#L569-L574"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "markdown\n",
    "# PPO（Proximal Policy Optimization）核心公式与实现\n",
    "\n",
    "PPO算法的核心是通过策略损失和价值损失的联合优化来更新智能体策略，以下是完整的公式说明与代码实现。\n",
    "\n",
    "## 1. 策略损失（Policy Loss）\n",
    "\n",
    "### 核心公式\n",
    "\n",
    "策略损失的计算基于重要性采样和裁剪机制：\n",
    "\n",
    "1. **重要性采样比率**  \n",
    "   $$\\text{ratio}_t = \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} = \\exp\\left(\\log \\pi_\\theta(a_t | s_t) - \\log \\pi_{\\theta_{\\text{old}}}(a_t | s_t)\\right)$$\n",
    "\n",
    "2. **未裁剪损失**  \n",
    "   $$L_1(\\theta) = -A_t \\cdot \\text{ratio}_t$$\n",
    "\n",
    "3. **裁剪后损失**  \n",
    "   $$L_2(\\theta) = -A_t \\cdot \\text{clip}(\\text{ratio}_t, 1-\\epsilon, 1+\\epsilon)$$\n",
    "\n",
    "4. **最终策略损失**  \n",
    "   $$L_{\\text{policy}}(\\theta) = \\mathbb{E}\\left[ \\max(L_1(\\theta), L_2(\\theta)) \\right]$$\n",
    "\n",
    "其中：\n",
    "- $A_t$ 是优势估计（GAE计算结果）\n",
    "- $\\epsilon$ 是裁剪范围超参数（通常为0.2）\n",
    "- $\\pi_\\theta$ 是当前策略，$\\pi_{\\theta_{\\text{old}}}$ 是更新前的旧策略\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_loss(advantages, logprobs, logprobs_old, mask, cliprange):\n",
    "    # 重要性采样\n",
    "    ratio = torch.exp(logprobs - logprobs_old)\n",
    "    # 计算策略损失\n",
    "    pg_losses = -advantages * ratio\n",
    "    pg_losses2 = -advantages * torch.clamp(ratio, 1.0 - cliprange, 1.0 + cliprange)\n",
    "    pg_loss_max = torch.max(pg_losses, pg_losses2)\n",
    "    pg_loss = masked_mean(pg_loss_max, mask)\n",
    "    return pg_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算策略损失（裁剪版 PPO surrogate）\n",
    "pg_loss = get_policy_loss(gae, ppo_old_batchs['logprobs'], ppo_old_batchs['logprobs_old'], ppo_old_batchs['mask'], ppo_config.cliprange_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4202, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算熵损失\n",
    "https://github.com/huggingface/trl/blob/26d86757a7c7e24e397ea44f57ecce6031dfac01/trl/trainer/ppo_trainer.py#L582-L583"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "entropy（熵）没有直接参与到模型的损失（loss）\n",
    "\n",
    "在计算完损失并进行反向传播和参数更新后，代码计算了 entropy\n",
    "\n",
    "这里计算的 entropy 被记录到 entropy_stats 张量中，用于后续的统计和记录，但没有用于损失计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_ids shape: torch.Size([3, 10])\n",
      "logits shape: torch.Size([3, 10, 50257])\n"
     ]
    }
   ],
   "source": [
    "# 生成当前 actor 的 logits，用于后续熵计算\n",
    "logits = get_logits(models.actor, ppo_old_batchs['response'])\n",
    "ppo_old_batchs['logits'] = logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO中的熵损失（Entropy Loss）计算\n",
    "\n",
    "熵损失用于衡量策略的随机性，在PPO中通常作为总损失的一部分，鼓励智能体保持探索行为。\n",
    "\n",
    "## 熵计算函数\n",
    "\n",
    "```python\n",
    "def get_entropy_loss(logits, mask):\n",
    "    # 将logits转换为概率分布（softmax归一化）\n",
    "    prob_dist = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # 计算熵: H(p) = -Σ(p_i * log(p_i))\n",
    "    # 等价于: log(Σ(exp(logits_i))) - Σ(p_i * logits_i)\n",
    "    entropy = torch.logsumexp(logits, dim=-1) - torch.sum(prob_dist * logits, dim=-1)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "# 计算旧批次数据的熵\n",
    "entropy = get_entropy_loss(ppo_old_batchs['logits'], ppo_old_batchs['mask'])\n",
    "entropy  # 返回每个样本的熵值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: torch.Size([3, 10, 50257]), mask shape: torch.Size([3, 10])\n",
      "prob_dist shape: torch.Size([3, 10, 50257]), logits shape: torch.Size([3, 10, 50257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[10.7993, 10.7995, 10.7994, 10.7994, 10.7990, 10.7992, 10.7994, 10.7994,\n",
       "         10.7997, 10.7995],\n",
       "        [10.7995, 10.7994, 10.7994, 10.7996, 10.7995, 10.7992, 10.7994, 10.7995,\n",
       "         10.7993, 10.7996],\n",
       "        [10.7992, 10.7996, 10.7994, 10.7993, 10.7995, 10.7993, 10.7994, 10.7994,\n",
       "         10.7996, 10.7994]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_entropy_loss(logits, mask):\n",
    "    # 计算策略熵：H = logsumexp - p*logits，衡量策略的随机性\n",
    "    prob_dist = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    print(f\"prob_dist shape: {prob_dist.shape}, logits shape: {logits.shape}\")\n",
    "    entropy = torch.logsumexp(logits, dim=-1) - torch.sum(prob_dist * logits, dim=-1)\n",
    "    return entropy\n",
    "\n",
    "print(f\"logits shape: {logits.shape}, mask shape: {ppo_old_batchs['mask'].shape}\")\n",
    "entropy = get_entropy_loss(ppo_old_batchs['logits'], ppo_old_batchs['mask'])\n",
    "entropy\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 总损失：策略损失 + vf_coef * 价值损失（未加入熵项，保持与示例一致）\n",
    "loss = pg_loss + ppo_config.vf_coef * value_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(batchs, ppo_config):\n",
    "    # 汇总单个小批次的 PPO 损失计算流程\n",
    "    gae = get_GAE(batchs['rewards_kl'],\n",
    "                  batchs['mask'],\n",
    "                  batchs['values'],\n",
    "                  ppo_config.gamma,\n",
    "                  ppo_config.lam)\n",
    "    value_loss = get_value_loss(gae,\n",
    "                             batchs['values'],\n",
    "                             batchs['values_old'],\n",
    "                             batchs['mask'],\n",
    "                             ppo_config.cliprange_value)\n",
    "    pg_loss = get_policy_loss(\n",
    "                              gae,\n",
    "                              batchs['logprobs'],\n",
    "                              batchs['logprobs_old'],\n",
    "                              batchs['mask'],\n",
    "                              ppo_config.cliprange_value)\n",
    "    entropy = get_entropy_loss(batchs['logits'], batchs['mask'])  # 仅统计，不计入总损失\n",
    "    loss = pg_loss + ppo_config.vf_coef * value_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_dist shape: torch.Size([3, 10, 50257]), logits shape: torch.Size([3, 10, 50257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9609, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 演示计算一次 batch 的总损失\n",
    "loss = get_loss(ppo_old_batchs, ppo_config)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': tensor([[5, 0, 0, 1, 0],\n",
       "         [4, 8, 1, 4, 1],\n",
       "         [9, 6, 7, 0, 5]]),\n",
       " 'response': tensor([[4, 8, 5, 2, 9, 5, 5, 0, 6, 3],\n",
       "         [0, 3, 0, 4, 8, 2, 6, 4, 9, 3],\n",
       "         [2, 6, 7, 5, 0, 0, 3, 3, 4, 8]]),\n",
       " 'mask': tensor([[0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]]),\n",
       " 'logprobs_ref': tensor([[ -9.7659,  -9.9431,  -9.7075,  -9.8018,  -9.6310,  -9.6916,  -9.7483,\n",
       "           -9.5755,  -9.7520,  -9.8097],\n",
       "         [ -9.9691,  -9.7657,  -9.7810,  -9.7806,  -9.8304,  -9.9382,  -9.6816,\n",
       "           -9.9212,  -9.7132,  -9.8413],\n",
       "         [-10.4189,  -9.7863, -10.1431,  -9.8084,  -9.5995,  -9.5113,  -9.8666,\n",
       "           -9.7238,  -9.6501,  -9.6926]], grad_fn=<SqueezeBackward1>),\n",
       " 'logprobs_old': tensor([[ -9.6364, -10.0382,  -9.4454,  -9.7810,  -9.3484,  -9.5437,  -9.6146,\n",
       "           -9.3174,  -9.8408,  -9.5032],\n",
       "         [ -9.6546,  -9.7166,  -9.7343,  -9.4578,  -9.8507,  -9.7604,  -9.8515,\n",
       "           -9.6053,  -9.3741,  -9.4720],\n",
       "         [ -9.8447, -10.2057,  -9.4921,  -9.7237,  -9.1873,  -9.4923,  -9.6284,\n",
       "           -9.9353,  -9.3172,  -9.8445]], grad_fn=<SqueezeBackward1>),\n",
       " 'logprobs': tensor([[ -9.6364, -10.0382,  -9.4454,  -9.7810,  -9.3484,  -9.5437,  -9.6146,\n",
       "           -9.3174,  -9.8408,  -9.5032],\n",
       "         [ -9.6546,  -9.7166,  -9.7343,  -9.4578,  -9.8507,  -9.7604,  -9.8515,\n",
       "           -9.6053,  -9.3741,  -9.4720],\n",
       "         [ -9.8447, -10.2057,  -9.4921,  -9.7237,  -9.1873,  -9.4923,  -9.6284,\n",
       "           -9.9353,  -9.3172,  -9.8445]], grad_fn=<SqueezeBackward1>),\n",
       " 'values_old': tensor([[ 0.1939, -0.0731, -0.0170, -0.4315,  0.0534, -0.2046, -0.6074, -0.7700,\n",
       "          -1.2505,  0.1553],\n",
       "         [ 0.0511, -0.2098, -0.8512, -0.1117,  0.2560, -0.0967, -0.9718,  0.2660,\n",
       "          -0.1777,  0.4735],\n",
       "         [ 0.2042, -0.6096, -0.0284,  0.2577, -0.3757, -0.3134, -0.5433, -0.2487,\n",
       "          -0.2369,  1.0747]], grad_fn=<SelectBackward0>),\n",
       " 'values': tensor([[ 0.6939,  0.4269,  0.4830,  0.0685,  0.5534,  0.2954, -0.1074, -0.2700,\n",
       "          -0.7505,  0.6553],\n",
       "         [ 0.5511,  0.2902, -0.3512,  0.3883,  0.7560,  0.4033, -0.4718,  0.7660,\n",
       "           0.3223,  0.9735],\n",
       "         [ 0.7042, -0.1096,  0.4716,  0.7577,  0.1243,  0.1866, -0.0433,  0.2513,\n",
       "           0.2631,  1.5747]], grad_fn=<AddBackward0>),\n",
       " 'rewards': tensor([[-0.7784],\n",
       "         [-0.9515],\n",
       "         [-0.9003]], grad_fn=<AddmmBackward0>),\n",
       " 'rewards_kl': tensor([[-0.0130,  0.0095, -0.0262, -0.0021, -0.0283, -0.0148, -0.0134, -0.0258,\n",
       "           0.0089, -0.8090],\n",
       "         [-0.0315, -0.0049, -0.0047, -0.0323,  0.0020, -0.0178,  0.0170, -0.0316,\n",
       "          -0.0339, -0.9884],\n",
       "         [-0.0574,  0.0419, -0.0651, -0.0085, -0.0412, -0.0019, -0.0238,  0.0211,\n",
       "          -0.0333, -0.8852]], grad_fn=<CopySlices>),\n",
       " 'loss': None,\n",
       " 'logits': tensor([[[-1.4843e-01, -3.8199e-01,  1.5566e-01,  ...,  6.0343e-01,\n",
       "           -3.5546e-01, -2.5944e-01],\n",
       "          [-1.6893e-01, -2.5384e-03, -8.4530e-03,  ...,  8.7142e-02,\n",
       "           -2.0942e-01, -8.3370e-02],\n",
       "          [-4.3086e-01,  5.5402e-02, -4.6384e-01,  ...,  9.1063e-02,\n",
       "           -8.1510e-02,  1.6532e-01],\n",
       "          ...,\n",
       "          [ 1.5315e+00, -8.8365e-02, -1.9262e-01,  ..., -2.3480e-01,\n",
       "            7.7313e-02, -1.3036e-02],\n",
       "          [-9.2542e-02, -2.2912e-01,  8.3747e-02,  ...,  2.8154e-03,\n",
       "           -1.3022e-01,  6.1364e-02],\n",
       "          [-4.0653e-01, -2.8789e-02, -1.5729e-01,  ...,  2.5900e-01,\n",
       "           -3.2773e-01, -1.3417e-01]],\n",
       " \n",
       "         [[ 1.1939e+00, -3.4385e-01,  1.8697e-01,  ...,  8.9561e-02,\n",
       "           -1.3423e-01, -5.1387e-05],\n",
       "          [ 1.3593e-01, -2.1616e-01,  1.7281e-01,  ...,  5.4955e-02,\n",
       "           -2.8100e-01, -9.6232e-02],\n",
       "          [ 1.1163e+00, -4.0199e-01, -5.8994e-02,  ..., -4.4124e-02,\n",
       "            8.6503e-02, -4.1281e-02],\n",
       "          ...,\n",
       "          [ 7.9734e-02, -4.3286e-01,  1.4872e-01,  ..., -5.1665e-03,\n",
       "           -7.4853e-02, -2.7805e-02],\n",
       "          [ 3.4729e-01, -2.8876e-01,  3.5831e-02,  ...,  1.3297e-01,\n",
       "           -8.0469e-03,  5.7139e-02],\n",
       "          [-3.4550e-01, -1.6689e-01, -1.2459e-01,  ...,  2.8532e-01,\n",
       "           -3.9113e-01, -1.1683e-01]],\n",
       " \n",
       "         [[ 6.7189e-03, -4.6148e-02,  1.0041e+00,  ...,  5.6802e-01,\n",
       "           -1.4841e-01, -1.4218e-01],\n",
       "          [-8.0866e-02, -2.3968e-01,  1.6320e-01,  ...,  6.1787e-02,\n",
       "            1.6179e-02,  2.5040e-01],\n",
       "          [-3.4248e-01, -1.3313e-01, -4.3621e-01,  ...,  3.2381e-01,\n",
       "            1.3221e-02,  5.6685e-02],\n",
       "          ...,\n",
       "          [ 3.4316e-01, -8.4548e-04, -3.4696e-01,  ..., -6.7568e-02,\n",
       "           -1.2948e-01, -1.6340e-01],\n",
       "          [-3.2091e-02, -6.8572e-01,  2.5836e-01,  ...,  2.4276e-01,\n",
       "           -1.0186e-01, -1.8865e-01],\n",
       "          [-4.6698e-01, -2.5016e-01, -1.1452e-01,  ...,  6.8086e-02,\n",
       "           -3.2970e-01, -7.7348e-02]]], grad_fn=<UnsafeViewBackward0>)}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_old_batchs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO训练\n",
    "\n",
    "https://github.com/huggingface/trl/blob/26d86757a7c7e24e397ea44f57ecce6031dfac01/trl/trainer/ppo_trainer.py#L529-L538"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将一个完整的批次数据 ppo_batchs 按照指定的 batch_size 和 mini_batch_size 划分成多个小批次数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_minibatch(ppo_batchs, batch_size, mini_batch_size):\n",
    "    # 将完整 batch 打乱并切成多个 mini-batch，用于多次 PPO 更新\n",
    "    step = batch_size // mini_batch_size\n",
    "    ppo_batchs_iter = []\n",
    "    \n",
    "    # 随机打乱索引以提高训练效果\n",
    "    b_inds = np.random.permutation(batch_size)\n",
    "    \n",
    "    # 根据索引创建小批次\n",
    "    for i in range(step):\n",
    "        start_idx = i * mini_batch_size\n",
    "        end_idx = start_idx + mini_batch_size\n",
    "        batch_inds = b_inds[start_idx:end_idx]\n",
    "        \n",
    "        # 创建当前小批次的数据；仅对 batch 维度匹配的张量做索引\n",
    "        mini_batch = {}\n",
    "        for key, value in ppo_batchs.items():\n",
    "            if value is not None and isinstance(value, torch.Tensor) and value.size(0) == batch_size:\n",
    "                mini_batch[key] = value[batch_inds]\n",
    "            else:\n",
    "                mini_batch[key] = value\n",
    "                \n",
    "        ppo_batchs_iter.append(mini_batch)\n",
    "    \n",
    "    return ppo_batchs_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示用 Adam 优化器；真实训练需考虑学习率调度与 weight decay 等\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': tensor([[5, 0, 0, 1, 0],\n",
       "         [4, 8, 1, 4, 1],\n",
       "         [9, 6, 7, 0, 5],\n",
       "         [4, 8, 5, 2, 9],\n",
       "         [5, 5, 0, 6, 3]]),\n",
       " 'response': tensor([[0, 3, 0, 4, 8, 2, 6, 4, 9, 3],\n",
       "         [2, 6, 7, 5, 0, 0, 3, 3, 4, 8],\n",
       "         [0, 8, 8, 2, 6, 0, 6, 0, 5, 8],\n",
       "         [8, 1, 4, 6, 2, 7, 5, 5, 9, 5],\n",
       "         [7, 4, 9, 5, 6, 6, 6, 1, 9, 8]]),\n",
       " 'mask': tensor([[0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]]),\n",
       " 'logprobs_ref': tensor([[ -9.7657,  -9.5145,  -9.7403,  -9.4521,  -9.8023,  -9.8455,  -9.8040,\n",
       "           -9.5040,  -9.9263,  -9.4373],\n",
       "         [-10.0543,  -9.8124,  -9.6533,  -9.7472,  -9.6888,  -9.7347,  -9.5207,\n",
       "           -9.2883,  -9.4406,  -9.7164],\n",
       "         [ -9.7657, -10.3167,  -9.8208,  -9.8356,  -9.5770,  -9.7337,  -9.7759,\n",
       "           -9.6341,  -9.4780,  -9.8436],\n",
       "         [-10.3158,  -9.5739,  -9.6799,  -9.8827, -10.0626,  -9.6075,  -9.7284,\n",
       "           -9.6707,  -9.9424,  -9.6236],\n",
       "         [ -9.8277,  -9.9490,  -9.4426,  -9.7313,  -9.5943,  -9.7917,  -9.6991,\n",
       "           -9.7685,  -9.9496,  -9.7640]], grad_fn=<SqueezeBackward1>),\n",
       " 'logprobs_old': tensor([[ -9.6546,  -9.7166,  -9.7343,  -9.4578,  -9.8507,  -9.7604,  -9.8515,\n",
       "           -9.6053,  -9.3741,  -9.4720],\n",
       "         [ -9.8447, -10.2057,  -9.4921,  -9.7237,  -9.1873,  -9.4923,  -9.6284,\n",
       "           -9.9353,  -9.3172,  -9.8445],\n",
       "         [ -9.6546, -10.1063,  -9.7419,  -9.6142,  -9.8585,  -9.5115,  -9.7855,\n",
       "           -9.2093,  -9.4475,  -9.7984],\n",
       "         [-10.0683,  -9.7843,  -9.6151,  -9.7731,  -9.4803,  -9.1821,  -9.4697,\n",
       "           -9.6959,  -9.3579,  -9.5344],\n",
       "         [ -9.6822,  -9.6050,  -9.5979,  -9.7321, -10.0195, -10.2095,  -9.9384,\n",
       "           -9.7428,  -9.4144,  -9.9008]], grad_fn=<SqueezeBackward1>),\n",
       " 'logprobs': tensor([[ -9.6546,  -9.7166,  -9.7343,  -9.4578,  -9.8507,  -9.7604,  -9.8515,\n",
       "           -9.6053,  -9.3741,  -9.4720],\n",
       "         [ -9.8447, -10.2057,  -9.4921,  -9.7237,  -9.1873,  -9.4923,  -9.6284,\n",
       "           -9.9353,  -9.3172,  -9.8445],\n",
       "         [ -9.6546, -10.1063,  -9.7419,  -9.6142,  -9.8585,  -9.5115,  -9.7855,\n",
       "           -9.2093,  -9.4475,  -9.7984],\n",
       "         [-10.0683,  -9.7843,  -9.6151,  -9.7731,  -9.4803,  -9.1821,  -9.4697,\n",
       "           -9.6959,  -9.3579,  -9.5344],\n",
       "         [ -9.6822,  -9.6050,  -9.5979,  -9.7321, -10.0195, -10.2095,  -9.9384,\n",
       "           -9.7428,  -9.4144,  -9.9008]], grad_fn=<SqueezeBackward1>),\n",
       " 'values_old': tensor([[ 1.2677,  0.5070,  0.9766, -0.4549,  0.5805, -0.4866,  0.5283, -0.2907,\n",
       "           0.0779, -0.1667],\n",
       "         [ 0.3226, -0.0667, -0.7088, -0.4413,  0.6490,  0.8188,  1.3689,  0.6129,\n",
       "           0.8584, -0.0860],\n",
       "         [ 1.2112,  0.0672,  0.4946, -0.7344,  0.5928,  0.8188,  1.0112,  0.7424,\n",
       "           1.3459, -0.0567],\n",
       "         [ 0.5810, -0.2458,  0.0620, -0.9607, -0.0040, -1.0716,  0.5418, -0.1127,\n",
       "          -0.0043, -0.3484],\n",
       "         [-0.4887, -0.2443, -0.6051, -0.6362,  0.2427, -0.0520,  0.6208,  0.1293,\n",
       "           0.1234, -0.2866]], grad_fn=<SelectBackward0>),\n",
       " 'values': tensor([[ 1.7677,  1.0070,  1.4766,  0.0451,  1.0805,  0.0134,  1.0283,  0.2093,\n",
       "           0.5779,  0.3333],\n",
       "         [ 0.8226,  0.4333, -0.2088,  0.0587,  1.1490,  1.3188,  1.8689,  1.1129,\n",
       "           1.3584,  0.4140],\n",
       "         [ 1.7112,  0.5672,  0.9946, -0.2344,  1.0928,  1.3188,  1.5112,  1.2424,\n",
       "           1.8459,  0.4433],\n",
       "         [ 1.0810,  0.2542,  0.5620, -0.4607,  0.4960, -0.5716,  1.0418,  0.3873,\n",
       "           0.4957,  0.1516],\n",
       "         [ 0.0113,  0.2557, -0.1051, -0.1362,  0.7427,  0.4480,  1.1208,  0.6293,\n",
       "           0.6234,  0.2134]], grad_fn=<AddBackward0>),\n",
       " 'rewards': tensor([[-0.9515],\n",
       "         [-0.9003],\n",
       "         [-1.3975],\n",
       "         [-1.6012],\n",
       "         [-1.6159]], grad_fn=<AddmmBackward0>),\n",
       " 'rewards_kl': tensor([[-1.1109e-02,  2.0212e-02, -5.9538e-04,  5.7230e-04,  4.8371e-03,\n",
       "          -8.5035e-03,  4.7553e-03,  1.0133e-02, -5.5222e-02, -9.4801e-01],\n",
       "         [-2.0961e-02,  3.9329e-02, -1.6113e-02, -2.3515e-03, -5.0153e-02,\n",
       "          -2.4239e-02,  1.0773e-02,  6.4699e-02, -1.2334e-02, -8.8754e-01],\n",
       "         [-1.1109e-02, -2.1040e-02, -7.8938e-03, -2.2135e-02,  2.8151e-02,\n",
       "          -2.2220e-02,  9.5730e-04, -4.2487e-02, -3.0542e-03, -1.4020e+00],\n",
       "         [-2.4752e-02,  2.1033e-02, -6.4787e-03, -1.0964e-02, -5.8229e-02,\n",
       "          -4.2533e-02, -2.5875e-02,  2.5274e-03, -5.8451e-02, -1.6101e+00],\n",
       "         [-1.4547e-02, -3.4400e-02,  1.5532e-02,  8.0109e-05,  4.2521e-02,\n",
       "           4.1776e-02,  2.3927e-02, -2.5682e-03, -5.3520e-02, -1.6023e+00]],\n",
       "        grad_fn=<CopySlices>),\n",
       " 'loss': None,\n",
       " 'logits': tensor([[[ 1.1939e+00, -3.4385e-01,  1.8697e-01,  ...,  8.9561e-02,\n",
       "           -1.3423e-01, -5.1387e-05],\n",
       "          [ 1.3593e-01, -2.1616e-01,  1.7281e-01,  ...,  5.4955e-02,\n",
       "           -2.8100e-01, -9.6232e-02],\n",
       "          [ 1.1163e+00, -4.0199e-01, -5.8994e-02,  ..., -4.4124e-02,\n",
       "            8.6503e-02, -4.1281e-02],\n",
       "          ...,\n",
       "          [ 7.9734e-02, -4.3286e-01,  1.4872e-01,  ..., -5.1665e-03,\n",
       "           -7.4853e-02, -2.7805e-02],\n",
       "          [ 3.4729e-01, -2.8876e-01,  3.5831e-02,  ...,  1.3297e-01,\n",
       "           -8.0469e-03,  5.7139e-02],\n",
       "          [-3.4550e-01, -1.6689e-01, -1.2459e-01,  ...,  2.8532e-01,\n",
       "           -3.9113e-01, -1.1683e-01]],\n",
       " \n",
       "         [[ 6.7189e-03, -4.6148e-02,  1.0041e+00,  ...,  5.6802e-01,\n",
       "           -1.4841e-01, -1.4218e-01],\n",
       "          [-8.0866e-02, -2.3968e-01,  1.6320e-01,  ...,  6.1787e-02,\n",
       "            1.6179e-02,  2.5040e-01],\n",
       "          [-3.4248e-01, -1.3313e-01, -4.3621e-01,  ...,  3.2381e-01,\n",
       "            1.3221e-02,  5.6685e-02],\n",
       "          ...,\n",
       "          [ 3.4316e-01, -8.4548e-04, -3.4696e-01,  ..., -6.7568e-02,\n",
       "           -1.2948e-01, -1.6340e-01],\n",
       "          [-3.2091e-02, -6.8572e-01,  2.5836e-01,  ...,  2.4276e-01,\n",
       "           -1.0186e-01, -1.8865e-01],\n",
       "          [-4.6698e-01, -2.5016e-01, -1.1452e-01,  ...,  6.8086e-02,\n",
       "           -3.2970e-01, -7.7348e-02]],\n",
       " \n",
       "         [[ 1.1939e+00, -3.4385e-01,  1.8697e-01,  ...,  8.9561e-02,\n",
       "           -1.3423e-01, -5.1387e-05],\n",
       "          [-1.0593e-01, -1.3282e-01,  2.0533e-01,  ..., -1.9474e-01,\n",
       "           -1.6972e-01,  4.7611e-02],\n",
       "          [-4.1880e-01,  1.8398e-02, -5.3639e-02,  ..., -1.0487e-02,\n",
       "           -1.2665e-01, -7.0815e-02],\n",
       "          ...,\n",
       "          [ 1.6399e+00, -2.6469e-01, -8.5538e-02,  ..., -2.8674e-01,\n",
       "            5.6738e-02,  8.3134e-02],\n",
       "          [ 1.7255e-01, -3.7670e-01, -3.0233e-01,  ..., -7.1360e-02,\n",
       "           -9.5127e-02,  4.1914e-01],\n",
       "          [-4.9126e-01, -2.2191e-01, -7.8555e-03,  ..., -5.6117e-03,\n",
       "           -3.6520e-01,  9.7580e-03]],\n",
       " \n",
       "         [[-1.4264e-01, -2.8157e-02,  2.0611e-01,  ...,  3.9266e-01,\n",
       "           -3.9834e-01, -2.0778e-01],\n",
       "          [-2.1525e-01,  1.0653e+00,  2.1692e-01,  ...,  1.1699e-01,\n",
       "            5.6338e-02, -1.0115e-01],\n",
       "          [-5.6471e-01, -2.6728e-01,  5.1792e-02,  ...,  2.3630e-01,\n",
       "           -8.6777e-02, -2.1680e-01],\n",
       "          ...,\n",
       "          [ 5.0904e-02,  6.5761e-02, -6.5508e-01,  ..., -3.1484e-01,\n",
       "            5.0776e-02,  3.6046e-01],\n",
       "          [ 2.1136e-01, -1.6706e-01, -4.8888e-02,  ...,  1.3312e-01,\n",
       "            2.5565e-03, -4.6409e-02],\n",
       "          [-5.2850e-01, -9.6140e-02, -4.2049e-01,  ...,  4.4030e-03,\n",
       "           -1.7598e-01,  2.3337e-01]],\n",
       " \n",
       "         [[-1.9936e-01, -1.6945e-01, -1.9695e-01,  ...,  5.2535e-01,\n",
       "           -1.7846e-01, -2.9423e-01],\n",
       "          [-3.5077e-01, -4.7752e-01,  2.0070e-01,  ...,  2.2220e-01,\n",
       "           -8.3356e-02, -2.5743e-01],\n",
       "          [-2.8892e-01,  3.0952e-02, -2.3381e-01,  ...,  1.5720e-01,\n",
       "            9.4805e-02, -8.3954e-02],\n",
       "          ...,\n",
       "          [ 6.9169e-02,  1.1067e+00, -2.3178e-01,  ...,  7.0888e-02,\n",
       "            2.1960e-01, -7.3331e-02],\n",
       "          [ 3.3634e-01, -3.3223e-01, -1.2819e-01,  ...,  1.1444e-01,\n",
       "            1.8477e-01, -8.0723e-02],\n",
       "          [-4.0076e-01, -2.4644e-01, -2.1143e-01,  ...,  1.5312e-02,\n",
       "           -1.8078e-01, -1.4051e-01]]], grad_fn=<UnsafeViewBackward0>)}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_old_batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_train_step(models, ppo_batchs, ppo_config, get_loss, optimizer):\n",
    "    # 以当前采样的 batch 为输入，执行多轮（ppo_epochs）PPO 更新；示例未包含重新采样/GAE 更新等完整逻辑\n",
    "    losses = []\n",
    "    \n",
    "    # 多轮PPO训练\n",
    "    for i in range(ppo_config.ppo_epochs):\n",
    "        # 获取小批次数据\n",
    "        ppo_batchs_iter = get_minibatch(\n",
    "            ppo_batchs, batch_size, ppo_config.mini_batch_size)\n",
    "        \n",
    "        # 对每个小批次进行训练\n",
    "        for mini_batchs in ppo_batchs_iter:\n",
    "            optimizer.zero_grad()\n",
    "            # 重新计算所有中间结果，而不是重用之前的计算图\n",
    "            with torch.set_grad_enabled(True):\n",
    "                # 这里应重新 forward 计算 logprobs/values/entropy 等；示例仅展示 logits 计算入口\n",
    "                logits = get_logits(models.actor, mini_batchs['prompt'])\n",
    "                \"\"\"\n",
    "                省略了：根据 mini_batchs 重新计算 logprobs/values/gae/rewards_kl 等，\n",
    "                以保证梯度针对当前模型参数。\n",
    "                \"\"\"\n",
    "\n",
    "                # 计算损失\n",
    "                loss = get_loss(\n",
    "                    mini_batchs, ppo_config)\n",
    "                \n",
    "                # 反向传播\n",
    "                loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 记录损失\n",
    "            losses.append(loss)\n",
    "    \n",
    "    # 更新批次数据中的损失\n",
    "    ppo_batchs['loss'] = losses\n",
    "    \n",
    "    print(losses)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
